<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Papers | Responsible Decision Making in Dynamic Environments - ICML 2022 Workshop</title>
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="Papers" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Workshop to discuss the current challenges and possible solutions of responsible sequential decision making." />
<meta property="og:description" content="Workshop to discuss the current challenges and possible solutions of responsible sequential decision making." />
<link rel="canonical" href="/papers/" />
<meta property="og:url" content="/papers/" />
<meta property="og:site_name" content="Responsible Decision Making in Dynamic Environments - ICML 2022 Workshop" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Papers" />
<script type="application/ld+json">
{"headline":"Papers","url":"/papers/","@type":"WebPage","description":"Workshop to discuss the current challenges and possible solutions of responsible sequential decision making.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="/feed.xml" title="Responsible Decision Making in Dynamic Environments - ICML 2022 Workshop" />
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
  
</head>
<body><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

<header class="site-header">
  <div class="wrapper"><!-- <a class="site-title" rel="author" href="/">Responsible Decision Making in Dynamic Environments - ICML 2022 Workshop</a> --><a class="site-title" rel="author" href="/">Responsible Decision Making in Dynamic Environments - ICML 2022 Workshop</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger">

<!--          <a href="https://twitter.com/responsibledec1" class="fa fa-twitter"></a>--><a class="page-link" href="/">Home</a><a class="page-link" href="/schedule/">Schedule</a><a class="page-link" href="/papers/">Papers</a><a class="page-link" href="/organizers/">Organizers</a><a class="page-link" href="/submit/">Submit</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">

	<header class="post-header">
		<h1 class="post-title">Papers</h1>
	</header>

	<div class = "post-content">

		


		<table>
			<tr>
				<td width="35%">
					<div class="people-name text-left">
						<b>Paper</b>
					</div>
				</td>
				<td width="50%">
					<div class="people-name text-left">
						<b>Details</b>
					</div>
				</td>
				<td width="15%">
					<div class="people-name text-left">
						<b>Video</b>
					</div>
				</td>
			</tr>
			
				<tr>
					<td width="35%">
						<div class="people-name text-left">
							<b>Combining Counterfactuals With Shapley Values To Explain Image Models</b>
							
							<!-- Author list -->
							
								<br>
								Lahiri, Aditya*; Alipour, Kamran; Adeli, Ehsan; Salimi, Babak
							
							<!-- Author affiliations (if provided) -->
							
<!--							&lt;!&ndash; Link to PDF (if provided) &ndash;&gt;-->
<!--							-->
<!--								<br>-->
<!--								<a href="/assets/pdf/papers/02.pdf" target="_blank">PDF</a>-->
<!--							-->
						</div>
					</td>
					<td width="50%">
						<div class="people-name text-left">
<!--							-->
							
								<button type="button" class="btn btn-outline-primary" data-toggle="collapse" data-target="#2_abstract">See Abstract</button>
								<br>
								<div id="2_abstract" class="collapse out">
									With the widespread use of sophisticated machine learning models in sensitive applications, understanding their decision-making has become an essential task. Models trained on tabular data have witnessed significant progress in explanations of their underlying decision making processes by virtue of having a small number of discrete features. However, applying these methods to high-dimensional inputs such as images is not a trivial task. Images are composed of pixels at an atomic level and do not carry any interpretability by themselves. In this work, we seek to use annotated high-level interpretable features of images to provide explanations. We leverage the Shapley value framework from Game Theory, which has garnered wide acceptance in general XAI problems. By developing a pipeline to generate counterfactuals and subsequently using it to estimate Shapley values, we obtain contrastive and interpretable explanations with strong axiomatic guarantees.
								</div>
							
<!--							&lt;!&ndash; Link to OpenReview (if provided) &ndash;&gt;-->
<!--							-->
							<!-- Link to PDF (if provided) -->
							
								<a class="btn btn-outline-primary" href="/assets/pdf/papers/02.pdf" role="button" target="_blank">Paper</a>
							
							<!-- Link to Poster (if provided) -->
							
								<a class="btn btn-outline-primary" href="/assets/poster/02.pdf" role="button" target="_blank">Poster</a>
							



						</div>
					</td>
					<td width="15%">
						<div class="people-name text-left">
							
								<iframe src="https://www.youtube.com/embed/QDhrtN5QuwM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
							
						</div>
					</td>
				</tr>
			
				<tr>
					<td width="35%">
						<div class="people-name text-left">
							<b>Individually Fair Learning with One-Sided Feedback</b>
							
								<span class="badge badge-secondary">Contributed talk</span>
							
							<!-- Author list -->
							
								<br>
								Bechavod, Yahav*; Roth, Aaron
							
							<!-- Author affiliations (if provided) -->
							
<!--							&lt;!&ndash; Link to PDF (if provided) &ndash;&gt;-->
<!--							-->
<!--								<br>-->
<!--								<a href="/assets/pdf/papers/04.pdf" target="_blank">PDF</a>-->
<!--							-->
						</div>
					</td>
					<td width="50%">
						<div class="people-name text-left">
<!--							-->
							
								<button type="button" class="btn btn-outline-primary" data-toggle="collapse" data-target="#4_abstract">See Abstract</button>
								<br>
								<div id="4_abstract" class="collapse out">
									We consider an online learning problem with one-sided feedback, in which the learner is able to observe the true label only for positively predicted instances. On each round, instances arrive and receive classification outcomes according to a randomized policy deployed by the learner, whose goal is to maximize accuracy while deploying individually fair policies. We first extend the framework of Bechavod et al. (2020), which relies on the existence of a human fairness auditor for detecting fairness violations, to instead incorporate feedback from dynamically-selected panels of multiple, possibly inconsistent, auditors. We then construct an efficient reduction from our problem of online learning with one-sided feedback and a panel reporting fairness violations to the contextual combinatorial semi-bandit problem (Cesa-Bianchi & Lugosi, 2009, Gyšrgy et al., 2007). Finally, we show how to leverage the guarantees of two algorithms in the contextual combinatorial semi-bandit setting Exp2 (Bubeck et al., 2012) and the oracle-efficient Context-Semi-Bandit-FTPL (Syrgkanis et al., 2016), to provide multi-criteria no regret guarantees simultaneously for accuracy and fairness. Our results resolve an open question of Bechavod et al. (2020), showing that individually fair and accurate online learning with auditor feedback can be carried out in the one-sided feedback setting.
								</div>
							
<!--							&lt;!&ndash; Link to OpenReview (if provided) &ndash;&gt;-->
<!--							-->
							<!-- Link to PDF (if provided) -->
							
								<a class="btn btn-outline-primary" href="/assets/pdf/papers/04.pdf" role="button" target="_blank">Paper</a>
							
							<!-- Link to Poster (if provided) -->
							



						</div>
					</td>
					<td width="15%">
						<div class="people-name text-left">
							
						</div>
					</td>
				</tr>
			
				<tr>
					<td width="35%">
						<div class="people-name text-left">
							<b>Robust Reinforcement Learning with Distributional Risk-averse formulation</b>
							
							<!-- Author list -->
							
								<br>
								Clavier, Pierre*; Allassonniere, Stephanie; LE PENNEC, Erwann
							
							<!-- Author affiliations (if provided) -->
							
<!--							&lt;!&ndash; Link to PDF (if provided) &ndash;&gt;-->
<!--							-->
<!--								<br>-->
<!--								<a href="/assets/pdf/papers/05.pdf" target="_blank">PDF</a>-->
<!--							-->
						</div>
					</td>
					<td width="50%">
						<div class="people-name text-left">
<!--							-->
							
								<button type="button" class="btn btn-outline-primary" data-toggle="collapse" data-target="#5_abstract">See Abstract</button>
								<br>
								<div id="5_abstract" class="collapse out">
									Robust Reinforcement Learning tries to make predictions more robust to changes in the dynamics or rewards of the system. This problem is particularly important when the dynamics and rewards of the environment are estimated from the data. In this paper, we approximate the Robust Reinforcement Learning constrained with a $\Phi$-divergence using an approximate Risk-Averse formulation. We show that the classical Reinforcement Learning formulation can be robustified using standard deviation penalization of the objective. Two algorithms based on Distributional Reinforcement Learning, one for discrete and one for continuous action spaces are proposed and tested in a classical Gym environment to demonstrate the robustness of the algorithms.
								</div>
							
<!--							&lt;!&ndash; Link to OpenReview (if provided) &ndash;&gt;-->
<!--							-->
							<!-- Link to PDF (if provided) -->
							
								<a class="btn btn-outline-primary" href="/assets/pdf/papers/05.pdf" role="button" target="_blank">Paper</a>
							
							<!-- Link to Poster (if provided) -->
							
								<a class="btn btn-outline-primary" href="/assets/poster/05.pdf" role="button" target="_blank">Poster</a>
							



						</div>
					</td>
					<td width="15%">
						<div class="people-name text-left">
							
								 <video src="https://user-images.githubusercontent.com/43342527/177513690-38202278-76db-42a5-9d68-2a159cfa80c7.mp4" controls="controls" style="max-width: 300px;" >
							
						</div>
					</td>
				</tr>
			
				<tr>
					<td width="35%">
						<div class="people-name text-left">
							<b>Optimal Dynamic Regret in LQR Control</b>
							
							<!-- Author list -->
							
								<br>
								Baby, Dheeraj*; Wang, Yu-Xiang
							
							<!-- Author affiliations (if provided) -->
							
<!--							&lt;!&ndash; Link to PDF (if provided) &ndash;&gt;-->
<!--							-->
<!--								<br>-->
<!--								<a href="/assets/pdf/papers/06.pdf" target="_blank">PDF</a>-->
<!--							-->
						</div>
					</td>
					<td width="50%">
						<div class="people-name text-left">
<!--							-->
							
								<button type="button" class="btn btn-outline-primary" data-toggle="collapse" data-target="#6_abstract">See Abstract</button>
								<br>
								<div id="6_abstract" class="collapse out">
									We consider the problem of nonstochastic control with a sequence of quadratic losses, i.e., LQR control. We provide an efficient online algorithm that achieves an optimal dynamic (policy) regret of $\tilde{O}(n^{1/3} \TV(M_{1:n}^{2/3}  \vee 1)$, where $\TV(M_{1:n})$ is the total variation of any oracle sequence of \emph{Disturbance Action} policies parameterized by $M_1,...,M_n$ --- chosen in hindsight to cater to unknown nonstationarity. The rate improves the best known rate of $\tilde{O}(\sqrt{n (\TV(M_{1:n})+1)} )$ for general convex losses and is information-theoretically optimal for LQR. Main technical components include the reduction of LQR to online linear regression with delayed feedback due to Foster and Simchowitz 2020, as well as a new \emph{proper} learning algorithm with an optimal $\tilde{O}(n^{1/3})$ dynamic regret on a family of ``minibatched'' quadratic losses, which could be of independent interest.
								</div>
							
<!--							&lt;!&ndash; Link to OpenReview (if provided) &ndash;&gt;-->
<!--							-->
							<!-- Link to PDF (if provided) -->
							
								<a class="btn btn-outline-primary" href="/assets/pdf/papers/06.pdf" role="button" target="_blank">Paper</a>
							
							<!-- Link to Poster (if provided) -->
							
								<a class="btn btn-outline-primary" href="/assets/poster/06.pdf" role="button" target="_blank">Poster</a>
							



						</div>
					</td>
					<td width="15%">
						<div class="people-name text-left">
							
						</div>
					</td>
				</tr>
			
				<tr>
					<td width="35%">
						<div class="people-name text-left">
							<b>Optimal Rates of (Locally) Differentially Private Heavy-tailed Multi-Armed Bandits</b>
							
								<span class="badge badge-secondary">Contributed talk</span>
							
							<!-- Author list -->
							
								<br>
								Wu, Yulian*; Tao, Youming; Zhao, Peng; Wang, Di
							
							<!-- Author affiliations (if provided) -->
							
<!--							&lt;!&ndash; Link to PDF (if provided) &ndash;&gt;-->
<!--							-->
<!--								<br>-->
<!--								<a href="/assets/pdf/papers/07.pdf" target="_blank">PDF</a>-->
<!--							-->
						</div>
					</td>
					<td width="50%">
						<div class="people-name text-left">
<!--							-->
							
								<button type="button" class="btn btn-outline-primary" data-toggle="collapse" data-target="#7_abstract">See Abstract</button>
								<br>
								<div id="7_abstract" class="collapse out">
									In this paper we investigate the problem of stochastic multi-armed bandits (MAB) in the (local) differential privacy (DP/LDP) model. Unlike previous results that assume bounded/sub-Gaussian reward distributions, we focus on the setting where each arm's reward distribution only has $(1+v)$-th moment with some $v\in (0, 1]$. In the first part, we study the problem in the central $\epsilon$-DP model. We first provide a near-optimal result by developing a private and robust Upper Confidence Bound (UCB) algorithm. Then, we improve the result via a private and robust version of the Successive Elimination (SE) algorithm. Finally, we establish the lower bound to show that the instance-dependent regret of our improved algorithm is optimal. In the second part, we study the problem in the $\epsilon$-LDP model. We propose an algorithm that can be seen as locally private and robust version of SE algorithm, which provably achieves (near) optimal rates for both instance-dependent and instance-independent regret. Our results reveal differences between the problem of private MAB with bounded/sub-Gaussian rewards and heavy-tailed rewards. To achieve these (near) optimal rates, we develop several new hard instances and private robust estimators as byproducts, which might be used to other related problems.
								</div>
							
<!--							&lt;!&ndash; Link to OpenReview (if provided) &ndash;&gt;-->
<!--							-->
							<!-- Link to PDF (if provided) -->
							
								<a class="btn btn-outline-primary" href="/assets/pdf/papers/07.pdf" role="button" target="_blank">Paper</a>
							
							<!-- Link to Poster (if provided) -->
							
								<a class="btn btn-outline-primary" href="/assets/poster/07.pdf" role="button" target="_blank">Poster</a>
							



						</div>
					</td>
					<td width="15%">
						<div class="people-name text-left">
							
								<iframe src="https://www.youtube.com/embed/gFarhetjmAI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
							
						</div>
					</td>
				</tr>
			
				<tr>
					<td width="35%">
						<div class="people-name text-left">
							<b>RISE - Robust Individualized Decision Learning with Sensitive Variables</b>
							
							<!-- Author list -->
							
								<br>
								Tan, Xiaoqing*; Qi, Zhengling; Seymour, Christopher; Tang, Lu
							
							<!-- Author affiliations (if provided) -->
							
<!--							&lt;!&ndash; Link to PDF (if provided) &ndash;&gt;-->
<!--							-->
<!--								<br>-->
<!--								<a href="/assets/pdf/papers/08.pdf" target="_blank">PDF</a>-->
<!--							-->
						</div>
					</td>
					<td width="50%">
						<div class="people-name text-left">
<!--							-->
							
								<button type="button" class="btn btn-outline-primary" data-toggle="collapse" data-target="#08_abstract">See Abstract</button>
								<br>
								<div id="08_abstract" class="collapse out">
									This paper introduces RISE, a robust individualized decision learning framework with sensitive variables, where sensitive variables are collectible data and important to the intervention decision, but their inclusion in decision making is prohibited due to reasons such as delayed availability or fairness concerns. The convention is to ignore these sensitive variables in learning decision rules, leading to significant uncertainty and bias. To address this, we propose a decision learning framework to incorporate sensitive variables during offline training but do not include them in the input of the learned decision rule during deployment. Specifically, from a causal perspective, the proposed framework intends to improve the worst-case outcomes of individuals caused by sensitive variables that are unavailable at the time of decision. Unlike most existing literature that uses mean-optimal objectives, the proposed learning framework robustifies sensitive variables via finding a newly defined quantile- or infimum-optimal decision rule for improving the worst-off group among all sensitive variable realizations. The reliable performance of the proposed method is demonstrated through synthetic experiments and three real-data applications.
								</div>
							
<!--							&lt;!&ndash; Link to OpenReview (if provided) &ndash;&gt;-->
<!--							-->
							<!-- Link to PDF (if provided) -->
							
								<a class="btn btn-outline-primary" href="/assets/pdf/papers/08.pdf" role="button" target="_blank">Paper</a>
							
							<!-- Link to Poster (if provided) -->
							
								<a class="btn btn-outline-primary" href="/assets/poster/08.pdf" role="button" target="_blank">Poster</a>
							



						</div>
					</td>
					<td width="15%">
						<div class="people-name text-left">
							
								<iframe src="https://www.youtube.com/embed/K7p8eK3OTuI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
							
						</div>
					</td>
				</tr>
			
				<tr>
					<td width="35%">
						<div class="people-name text-left">
							<b>Acting Optimistically in Choosing Safe Actions</b>
							
							<!-- Author list -->
							
								<br>
								Chen, Tianrui*; Gangrade, Aditya; Saligrama, Venkatesh
							
							<!-- Author affiliations (if provided) -->
							
<!--							&lt;!&ndash; Link to PDF (if provided) &ndash;&gt;-->
<!--							-->
<!--								<br>-->
<!--								<a href="/assets/pdf/papers/10.pdf" target="_blank">PDF</a>-->
<!--							-->
						</div>
					</td>
					<td width="50%">
						<div class="people-name text-left">
<!--							-->
							
								<button type="button" class="btn btn-outline-primary" data-toggle="collapse" data-target="#10_abstract">See Abstract</button>
								<br>
								<div id="10_abstract" class="collapse out">
									We investigate a natural but surprisingly unstudied approach to the multi-armed bandit problem under safety risk constraints. Each arm is associated with an unknown law on safety risks and rewards, and the learner's goal is to maximise reward whilst not playing unsafe arms, as determined by a given threshold on the mean risk. We formulate a pseudo-regret for this setting that enforces this safety constraint in a per-round way by softly penalising any violation, regardless of the gain in reward due to the same. This has practical relevance to scenarios such as clinical trials, where one must maintain safety for each round rather than in an aggregated sense. We describe doubly optimistic strategies for this scenario, which maintain optimistic indices for both safety risk and reward. We show that schema based on both frequentist and Bayesian indices satisfy tight gap-dependent logarithmic regret bounds, and further that these play unsafe arms only logarithmically many times in total. This theoretical analysis is complemented by simulation studies demonstrating the effectiveness of the proposed schema, and probing the domains in which their use is appropriate
								</div>
							
<!--							&lt;!&ndash; Link to OpenReview (if provided) &ndash;&gt;-->
<!--							-->
							<!-- Link to PDF (if provided) -->
							
								<a class="btn btn-outline-primary" href="/assets/pdf/papers/10.pdf" role="button" target="_blank">Paper</a>
							
							<!-- Link to Poster (if provided) -->
							



						</div>
					</td>
					<td width="15%">
						<div class="people-name text-left">
							
								<iframe src="https://www.youtube.com/embed/IE7pJY9Cq9s" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
							
						</div>
					</td>
				</tr>
			
				<tr>
					<td width="35%">
						<div class="people-name text-left">
							<b>Dynamic Positive Reinforcement For Long-Term Fairness</b>
							
							<!-- Author list -->
							
								<br>
								Puranik, Bhagyashree*; Madhow, Upamanyu; Pedarsani, Ramtin
							
							<!-- Author affiliations (if provided) -->
							
<!--							&lt;!&ndash; Link to PDF (if provided) &ndash;&gt;-->
<!--							-->
<!--								<br>-->
<!--								<a href="/assets/pdf/papers/11.pdf" target="_blank">PDF</a>-->
<!--							-->
						</div>
					</td>
					<td width="50%">
						<div class="people-name text-left">
<!--							-->
							
								<button type="button" class="btn btn-outline-primary" data-toggle="collapse" data-target="#11_abstract">See Abstract</button>
								<br>
								<div id="11_abstract" class="collapse out">
									As AI-based decision-making becomes increasingly impactful on human society, the study of the influence of fairness-aware policies on the population becomes important. In this work, we propose a framework for sequential decision-making aimed at dynamically influencing long-term societal fairness, illustrated via the problem of selecting applicants from a pool consisting of two groups, one of which is under-represented. We consider a dynamic model for the composition of the applicant pool, where the admission of more applicants from a particular group positively reinforces more such candidates to participate in the selection process. Under such a model, we show the efficacy of the proposed Fair-Greedy selection policy which systematically trades greedy score maximization against fairness objectives. In addition to experimenting on synthetic data, we adapt static real-world datasets on law school candidates and credit lending to simulate the dynamics of the composition of the applicant pool.
								</div>
							
<!--							&lt;!&ndash; Link to OpenReview (if provided) &ndash;&gt;-->
<!--							-->
							<!-- Link to PDF (if provided) -->
							
								<a class="btn btn-outline-primary" href="/assets/pdf/papers/11.pdf" role="button" target="_blank">Paper</a>
							
							<!-- Link to Poster (if provided) -->
							



						</div>
					</td>
					<td width="15%">
						<div class="people-name text-left">
							
								 <a class="btn btn-outline-primary" href="https://ucsb.app.box.com/s/04ezrpf3b9o3caeja511ow2a59ebwp5m" role="button" target="_blank">Video</a>
							
						</div>
					</td>
				</tr>
			
				<tr>
					<td width="35%">
						<div class="people-name text-left">
							<b>An Investigation into the Open World Survival Game Crafter</b>
							
							<!-- Author list -->
							
								<br>
								Stanic, Aleksandar*; Tang, Yujin; Ha, David; Schmidhuber, JŸrgen
							
							<!-- Author affiliations (if provided) -->
							
<!--							&lt;!&ndash; Link to PDF (if provided) &ndash;&gt;-->
<!--							-->
<!--								<br>-->
<!--								<a href="/assets/pdf/papers/13.pdf" target="_blank">PDF</a>-->
<!--							-->
						</div>
					</td>
					<td width="50%">
						<div class="people-name text-left">
<!--							-->
							
								<button type="button" class="btn btn-outline-primary" data-toggle="collapse" data-target="#13_abstract">See Abstract</button>
								<br>
								<div id="13_abstract" class="collapse out">
									We share our experience with the recently released Crafter benchmark, a 2D open world survival game. Crafter allows tractable investigation of novel agents and their generalization, exploration and long-term reasoning capabilities. We evaluate agents on the original Crafter environment, as well as on a newly introduced set of generalization environments, suitable for evaluating agents' robustness to unseen objects and fast-adaptation (meta-learning) capabilities. Through several experiments we provide a couple of critical insights that are of general interest for future work on Crafter. We find that- (1) Simple agents with tuned hyper-parameters outperform all previous agents. (2) Feedforward agents can unlock almost all achievements by relying on the inventory display. (3) Recurrent agents improve on feedforward ones, also without the inventory information. (4) All agents (including interpretable object-centric ones) fail to generalize to OOD objects. We will open-source our code.
								</div>
							
<!--							&lt;!&ndash; Link to OpenReview (if provided) &ndash;&gt;-->
<!--							-->
							<!-- Link to PDF (if provided) -->
							
								<a class="btn btn-outline-primary" href="/assets/pdf/papers/13.pdf" role="button" target="_blank">Paper</a>
							
							<!-- Link to Poster (if provided) -->
							



						</div>
					</td>
					<td width="15%">
						<div class="people-name text-left">
							
								 <a class="btn btn-outline-primary" href="https://drive.google.com/file/d/1qIRspEB6G1fPPbdvaVEytO8S_zLRHIlf/view?usp=sharings" role="button" target="_blank">Video</a>
							
						</div>
					</td>
				</tr>
			
				<tr>
					<td width="35%">
						<div class="people-name text-left">
							<b>Equity and Equality in Fair Federated Learning</b>
							
							<!-- Author list -->
							
								<br>
								Mozaffari, Hamid*; Houmansadr, Amir
							
							<!-- Author affiliations (if provided) -->
							
<!--							&lt;!&ndash; Link to PDF (if provided) &ndash;&gt;-->
<!--							-->
<!--								<br>-->
<!--								<a href="/assets/pdf/papers/15.pdf" target="_blank">PDF</a>-->
<!--							-->
						</div>
					</td>
					<td width="50%">
						<div class="people-name text-left">
<!--							-->
							
								<button type="button" class="btn btn-outline-primary" data-toggle="collapse" data-target="#15_abstract">See Abstract</button>
								<br>
								<div id="15_abstract" class="collapse out">
									Federated Learning (FL) enables data owners to train a shared global model without sharing their private data. Unfortunately, FL is susceptible to an intrinsic fairness issue- due to heterogeneity in clients' data distributions, the final trained model can give disproportionate advantages across the participating clients. In this work, we present Equal and Equitable Federated Learning (E2FL) to produce fair federated learning models by preserving two main fairness properties,   equity and equality, concurrently. We validate the efficiency and fairness of E2FL in different real-world FL applications, and show that E2FL outperforms existing baselines in terms of the resulting efficiency, fairness of different groups, and fairness among all individual clients.
								</div>
							
<!--							&lt;!&ndash; Link to OpenReview (if provided) &ndash;&gt;-->
<!--							-->
							<!-- Link to PDF (if provided) -->
							
								<a class="btn btn-outline-primary" href="/assets/pdf/papers/15.pdf" role="button" target="_blank">Paper</a>
							
							<!-- Link to Poster (if provided) -->
							
								<a class="btn btn-outline-primary" href="/assets/poster/15.pdf" role="button" target="_blank">Poster</a>
							



						</div>
					</td>
					<td width="15%">
						<div class="people-name text-left">
							
								<iframe src="https://www.youtube.com/embed/R3vSFDODpF0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
							
						</div>
					</td>
				</tr>
			
				<tr>
					<td width="35%">
						<div class="people-name text-left">
							<b>Certifiably Robust Multi-Agent Reinforcement Learning against Adversarial Communication</b>
							
							<!-- Author list -->
							
								<br>
								Sun, Yanchao*; Zheng, Ruijie; Hassanzadeh, Parisa; Liang, Yongyuan; Feizi, Soheil; Ganesh, Sumitra; Huang, Furong
							
							<!-- Author affiliations (if provided) -->
							
<!--							&lt;!&ndash; Link to PDF (if provided) &ndash;&gt;-->
<!--							-->
<!--								<br>-->
<!--								<a href="/assets/pdf/papers/16.pdf" target="_blank">PDF</a>-->
<!--							-->
						</div>
					</td>
					<td width="50%">
						<div class="people-name text-left">
<!--							-->
							
								<button type="button" class="btn btn-outline-primary" data-toggle="collapse" data-target="#16_abstract">See Abstract</button>
								<br>
								<div id="16_abstract" class="collapse out">
									Communication is important in many multi-agent reinforcement learning (MARL) problems for agents to share information and make good decisions. However, when deploying trained communicative agents in a real-world application where noise and potential attackers exist, the safety of communication-based policies becomes a severe issue that is underexplored. Specifically, if communication messages are manipulated by malicious attackers, agents relying on untrustworthy communication may take unsafe actions that lead to catastrophic consequences. Therefore, it is crucial to ensure that agents will not be misled by corrupted communication, while still benefiting from benign communication. In this work, we consider an environment with $N$ agents, where the attacker may arbitrarily change the communication from any $C<\frac{N-1}{2}$ agents to a victim agent. For this strong threat model, we propose a certifiable defense by constructing a message-ensemble policy that aggregates multiple randomly ablated message sets. Theoretical analysis shows that this message-ensemble policy can utilize benign communication while being certifiably robust to adversarial communication, regardless of the attacking algorithm. Experiments in multiple environments verify that our defense significantly improves the robustness of trained policies against various types of attacks.
								</div>
							
<!--							&lt;!&ndash; Link to OpenReview (if provided) &ndash;&gt;-->
<!--							-->
							<!-- Link to PDF (if provided) -->
							
								<a class="btn btn-outline-primary" href="/assets/pdf/papers/16.pdf" role="button" target="_blank">Paper</a>
							
							<!-- Link to Poster (if provided) -->
							
								<a class="btn btn-outline-primary" href="/assets/poster/16.pdf" role="button" target="_blank">Poster</a>
							



						</div>
					</td>
					<td width="15%">
						<div class="people-name text-left">
							
								 <a class="btn btn-outline-primary" href="https://drive.google.com/file/d/1vG67dSRNTE-u2gWvizhQh71jQytgxrJf/view?usp=sharing" role="button" target="_blank">Video</a>
							
						</div>
					</td>
				</tr>
			
				<tr>
					<td width="35%">
						<div class="people-name text-left">
							<b>Prisoners of Their Own Devices - How Models Induce Data Bias in Performative Prediction</b>
							
							<!-- Author list -->
							
								<br>
								Pombal, Jose*; Saleiro, Pedro; Figueiredo , Mario; Bizarro, Pedro
							
							<!-- Author affiliations (if provided) -->
							
<!--							&lt;!&ndash; Link to PDF (if provided) &ndash;&gt;-->
<!--							-->
<!--								<br>-->
<!--								<a href="/assets/pdf/papers/17.pdf" target="_blank">PDF</a>-->
<!--							-->
						</div>
					</td>
					<td width="50%">
						<div class="people-name text-left">
<!--							-->
							
								<button type="button" class="btn btn-outline-primary" data-toggle="collapse" data-target="#17_abstract">See Abstract</button>
								<br>
								<div id="17_abstract" class="collapse out">
									The unparalleled ability of machine learning algorithms to learn patterns from data also enables them to incorporate biases embedded within. A biased model can then make decisions that disproportionately harm certain groups in society. Much work has been devoted to measuring unfairness in static ML environments, but not in dynamic, performative prediction ones, in which most real- world use cases operate. In the latter, the predictive model itself plays a pivotal role in shaping the distribution of the data. However, little attention has been heeded to relating unfairness to these interactions. Thus, to further the understanding of unfairness in these settings, we propose a taxonomy to characterize bias in the data, and study cases where it is shaped by model behaviour. Using a real-world account opening fraud detection case study as an example, we explore the dangers to both performance and fairness of two typical biases in performative prediction - distribution shifts, and the problem of selective labels.
								</div>
							
<!--							&lt;!&ndash; Link to OpenReview (if provided) &ndash;&gt;-->
<!--							-->
							<!-- Link to PDF (if provided) -->
							
								<a class="btn btn-outline-primary" href="/assets/pdf/papers/17.pdf" role="button" target="_blank">Paper</a>
							
							<!-- Link to Poster (if provided) -->
							
								<a class="btn btn-outline-primary" href="/assets/poster/17.pdf" role="button" target="_blank">Poster</a>
							



						</div>
					</td>
					<td width="15%">
						<div class="people-name text-left">
							
								<iframe src="https://www.youtube.com/embed/MUQwmMtHkGo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
							
						</div>
					</td>
				</tr>
			
				<tr>
					<td width="35%">
						<div class="people-name text-left">
							<b>A Decision Metric for the Use of a Deep Reinforcement Learning Policy</b>
							
							<!-- Author list -->
							
								<br>
								Selby, Christina*
							
							<!-- Author affiliations (if provided) -->
							
<!--							&lt;!&ndash; Link to PDF (if provided) &ndash;&gt;-->
<!--							-->
<!--								<br>-->
<!--								<a href="/assets/pdf/papers/18.pdf" target="_blank">PDF</a>-->
<!--							-->
						</div>
					</td>
					<td width="50%">
						<div class="people-name text-left">
<!--							-->
							
								<button type="button" class="btn btn-outline-primary" data-toggle="collapse" data-target="#18_abstract">See Abstract</button>
								<br>
								<div id="18_abstract" class="collapse out">
									Uncertainty estimation techniques such as those found in Osband et al. (2018) and Burda et al. (2019) have been shown to be useful for efficient exploration during training.  This paper demonstrates that such uncertainty estimation techniques can also be used as part of a time-series based methodology for out-of-distribution (OOD) detection for an off-line model-free deep reinforcement learning policy.  In particular, this paper defines a "decision metric" that can be utilized for determining when another decision-making process should be used in place of the deep reinforcement learning policy.
								</div>
							
<!--							&lt;!&ndash; Link to OpenReview (if provided) &ndash;&gt;-->
<!--							-->
							<!-- Link to PDF (if provided) -->
							
								<a class="btn btn-outline-primary" href="/assets/pdf/papers/18.pdf" role="button" target="_blank">Paper</a>
							
							<!-- Link to Poster (if provided) -->
							
								<a class="btn btn-outline-primary" href="/assets/poster/18.pdf" role="button" target="_blank">Poster</a>
							



						</div>
					</td>
					<td width="15%">
						<div class="people-name text-left">
							
								<iframe src="https://www.youtube.com/embed/CCPqp0zJMf0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
							
						</div>
					</td>
				</tr>
			
				<tr>
					<td width="35%">
						<div class="people-name text-left">
							<b>Safe and Robust Experience Sharing for Deterministic Policy Gradient Algorithms</b>
							
							<!-- Author list -->
							
								<br>
								Sa_lam, Baturay*; Cicek, Dogan Can; Mutlu, Furkan Burak; Kozat, Suleyman S
							
							<!-- Author affiliations (if provided) -->
							
<!--							&lt;!&ndash; Link to PDF (if provided) &ndash;&gt;-->
<!--							-->
<!--								<br>-->
<!--								<a href="/assets/pdf/papers/19.pdf" target="_blank">PDF</a>-->
<!--							-->
						</div>
					</td>
					<td width="50%">
						<div class="people-name text-left">
<!--							-->
							
								<button type="button" class="btn btn-outline-primary" data-toggle="collapse" data-target="#19_abstract">See Abstract</button>
								<br>
								<div id="19_abstract" class="collapse out">
									Learning in high dimensional continuous tasks is challenging, mainly when the experience replay memory is very limited. We introduce a simple yet effective experience sharing mechanism for deterministic policies in continuous action domains for the future off-policy deep reinforcement learning applications in which the allocated memory for the experience replay buffer is limited. To overcome the extrapolation error induced by learning from other agents' experiences, we facilitate our algorithm with a novel off-policy correction technique without any action probability estimates. We test the effectiveness of our method in challenging OpenAI Gym continuous control tasks and conclude that it can achieve a safe experience sharing across multiple agents and exhibits a robust performance when the replay memory is strictly limited.
								</div>
							
<!--							&lt;!&ndash; Link to OpenReview (if provided) &ndash;&gt;-->
<!--							-->
							<!-- Link to PDF (if provided) -->
							
								<a class="btn btn-outline-primary" href="/assets/pdf/papers/19.pdf" role="button" target="_blank">Paper</a>
							
							<!-- Link to Poster (if provided) -->
							
								<a class="btn btn-outline-primary" href="/assets/poster/19.pdf" role="button" target="_blank">Poster</a>
							



						</div>
					</td>
					<td width="15%">
						<div class="people-name text-left">
							
								 <a class="btn btn-outline-primary" href="https://drive.google.com/file/d/1vjjMh_z51xdOjsQCcGfU5ojAcrrf3dOS/view?usp=sharing" role="button" target="_blank">Video</a>
							
						</div>
					</td>
				</tr>
			
				<tr>
					<td width="35%">
						<div class="people-name text-left">
							<b>Base</b>
							
							<!-- Author list -->
							
							<!-- Author affiliations (if provided) -->
							
<!--							&lt;!&ndash; Link to PDF (if provided) &ndash;&gt;-->
<!--							-->
						</div>
					</td>
					<td width="50%">
						<div class="people-name text-left">
<!--							-->
							
<!--							&lt;!&ndash; Link to OpenReview (if provided) &ndash;&gt;-->
<!--							-->
							<!-- Link to PDF (if provided) -->
							
							<!-- Link to Poster (if provided) -->
							



						</div>
					</td>
					<td width="15%">
						<div class="people-name text-left">
							
						</div>
					</td>
				</tr>
			
		</table>
	</div>

</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
<!--      <div class="footer-col">-->
<!--        <p class="feed-subscribe">-->
<!--          <a href="/feed.xml">-->
<!--            <svg class="svg-icon orange" role="img">-->
<!--              <title id="titleSocialIcons">social icons</title>-->
<!--              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>-->
<!--            </svg><span>Subscribe</span>-->
<!--          </a>-->
<!--        </p>-->
<!---->
<!--      </div>-->
      <div class="footer-col">
        <p>Workshop to discuss the current challenges and possible solutions of responsible sequential decision making.</p>
        <p>Based on a <a href="http://jekyllrb.com/">Jekyll</a> template from a <a href="https://krrish94.github.io">lazy grad student</a>. Some icons made by <a href="https://smashicons.com/" title="Smashicons">Smashicons</a> from <a href="https://www.flaticon.com/" title="Flaticon">www.flaticon.com</a>.
</p>
        
        Last updated: July 18, 2022.
        
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"></ul>
</div>

  </div>

</footer>
</body>

</html>
