I"4:<!-- 
<figure>
	<div style="text-align:center">
		<img src="assets/img/banner.png" alt="A logo image for the rethinking ML papers workshop, designed by Falaah Arif Khan" />
		<figcaption>Image Credits: <a href="https://falaaharifkhan.github.io/research/">Falaah Arif Khan</a></figcaption>
	</div>
</figure>
-->

<p>Algorithmic decision-making systems are increasingly used in sensitive applications such as advertising, resume reviewing, employment, credit lending, policing, criminal justice, and beyond. The long-term promise of these approaches is to automate, augment and/or eventually improve on the human decisions which can be biased or unfair, by leveraging the potential of machine learning to make decisions supported by historical data. Unfortunately, there is a growing body of evidence showing that the current machine learning technology is vulnerable to privacy or security attacks, lacks interpretability, or reproduces (and even exacerbates) historical biases or discriminatory behaviors against certain social groups.</p>

<p>Most of the literature on building socially responsible algorithmic decision-making systems focus on a static scenario where algorithmic decisions do not change the data distribution. However, real-world applications involve nonstationarities and feedback loops that must be taken into account to measure and mitigate fairness in the long-term. These feedback loops involve the learning process which may be biased because of insufficient exploration, or changes in the environment’s dynamics due to strategic responses of the various stakeholders. From a machine learning perspective, these sequential processes are primarily studied through counterfactual analysis and reinforcement learning.</p>

<p>The purpose of this workshop is to bring together researchers from both industry and academia working on the full spectrum of responsible decision-making in dynamic environments, from theory to practice. In particular, we encourage submissions on the following topics:</p>
<ul>
  <li>Fairness,</li>
  <li>Privacy and security,</li>
  <li>Robustness,</li>
  <li>Conservative and safe algorithms,</li>
  <li>Explainability and interpretability.</li>
</ul>

<h3 id="invited-speakers">Invited Speakers</h3>

<figure>
	<div class="post-content">
	  <table style="border-collapse: collapse; border: none;">
	  	
		    <tr style="border: none;">
		        <td style="border: none;">
		            <div class="col-xs-6">
		                <p align="center">
		                	
		                    	<img class="people-pic" src="/assets/img/speakers/aaron.jpeg" target="_blank" />
		                    
		                </p>
		            </div>
		        </td>
    		    <td style="border: none;">
		            <div class="people-name text-center">
		            	<!-- Speaker name (link to webpage if provided) -->
		            	
		            		<b><a href="https://www.cis.upenn.edu/~aaroth/" target="_blank">Aaron Roth</a></b>
		            	
		                <br />
		                <!-- Speaker affiliation (if provided) -->
		                
		                	University of Pennsylvania
		                
		                <!-- Additional speaker affiliation (if provided) -->
		                
		            </div>
		        </td>
		        <td style="border: none;">
		        	<div class="people-name text-center">
		        		<!-- Whatever you write below will show up as the speaker's bio -->

<p>Aaron Roth is the Henry Salvatori Professor of Computer and Cognitive Science at the University of Pennsylvania computer science department. He received his PhD from Carnegie Mellon University. His main interests are in algorithms and machine learning, and specifically in the areas of private data analysis, fairness in machine learning, game theory and mechanism design, and learning theory.</p>

		        	</div>
		        </td>
		    </tr>
	    
		    <tr style="border: none;">
		        <td style="border: none;">
		            <div class="col-xs-6">
		                <p align="center">
		                	
		                    	<img class="people-pic" src="/assets/img/speakers/craig.jpeg" target="_blank" />
		                    
		                </p>
		            </div>
		        </td>
    		    <td style="border: none;">
		            <div class="people-name text-center">
		            	<!-- Speaker name (link to webpage if provided) -->
		            	
		            		<b><a href="https://research.google/people/CraigBoutilier/" target="_blank">Craig Boutilier</a></b>
		            	
		                <br />
		                <!-- Speaker affiliation (if provided) -->
		                
		                	Google
		                
		                <!-- Additional speaker affiliation (if provided) -->
		                
		            </div>
		        </td>
		        <td style="border: none;">
		        	<div class="people-name text-center">
		        		<!-- Whatever you write below will show up as the speaker's bio -->

<p>Craig Boutilier is Principal Scientist at Google. He was a Professor in the Department of Computer Science at the University of Toronto (on leave) and Canada Research Chair in Adaptive Decision Making for Intelligent Systems. His current research efforts focus on various aspects of decision making under uncertainty: preference elicitation, mechanism design, game theory and multiagent decision processes, economic models, social choice, computational advertising, Markov decision processes, reinforcement learning and probabilistic inference.</p>

		        	</div>
		        </td>
		    </tr>
	    
		    <tr style="border: none;">
		        <td style="border: none;">
		            <div class="col-xs-6">
		                <p align="center">
		                	
		                    	<img class="people-pic" src="/assets/img/speakers/cynthia.jpeg" target="_blank" />
		                    
		                </p>
		            </div>
		        </td>
    		    <td style="border: none;">
		            <div class="people-name text-center">
		            	<!-- Speaker name (link to webpage if provided) -->
		            	
		            		<b><a href="https://users.cs.duke.edu/~cynthia/" target="_blank">Cynthia Rudin</a></b>
		            	
		                <br />
		                <!-- Speaker affiliation (if provided) -->
		                
		                	Duke University
		                
		                <!-- Additional speaker affiliation (if provided) -->
		                
		            </div>
		        </td>
		        <td style="border: none;">
		        	<div class="people-name text-center">
		        		<!-- Whatever you write below will show up as the speaker's bio -->

<p>Cynthia Rudin is a professor of computer science, electrical and computer engineering, statistical science, mathematics, and biostatistics &amp; bioinformatics at Duke University. She directs the Interpretable Machine Learning Lab, whose goal is to design predictive models with reasoning processes that are understandable to humans. Her lab applies machine learning in many areas, such as healthcare, criminal justice, and energy reliability. She holds an undergraduate degree from the University at Buffalo, and a PhD from Princeton University. She is the recipient of the 2022 Squirrel AI Award for Artificial Intelligence for the Benefit of Humanity from the Association for the Advancement of Artificial Intelligence (the “Nobel Prize of AI”). She is a fellow of the American Statistical Association, the Institute of Mathematical Statistics, and the Association for the Advancement of Artificial Intelligence. Her work has been featured in many news outlets including the NY Times, Washington Post, Wall Street Journal, and Boston Globe.</p>

		        	</div>
		        </td>
		    </tr>
	    
		    <tr style="border: none;">
		        <td style="border: none;">
		            <div class="col-xs-6">
		                <p align="center">
		                	
		                    	<img class="people-pic" src="/assets/img/speakers/finale.jpeg" target="_blank" />
		                    
		                </p>
		            </div>
		        </td>
    		    <td style="border: none;">
		            <div class="people-name text-center">
		            	<!-- Speaker name (link to webpage if provided) -->
		            	
		            		<b><a href="https://finale.seas.harvard.edu/" target="_blank">Finale Doshi-Velez</a></b>
		            	
		                <br />
		                <!-- Speaker affiliation (if provided) -->
		                
		                	Harvard University
		                
		                <!-- Additional speaker affiliation (if provided) -->
		                
		            </div>
		        </td>
		        <td style="border: none;">
		        	<div class="people-name text-center">
		        		<!-- Whatever you write below will show up as the speaker's bio -->

<p>Finale Doshi-Velez is a Gordon McKay Professor in Computer Science at the Harvard Paulson School of Engineering and Applied Sciences.  She completed her MSc from the University of Cambridge as a Marshall Scholar, her PhD from MIT, and her postdoc at Harvard Medical School.  Her interests lie at the intersection of machine learning, healthcare, and interpretability.</p>

		        	</div>
		        </td>
		    </tr>
	    
		    <tr style="border: none;">
		        <td style="border: none;">
		            <div class="col-xs-6">
		                <p align="center">
		                	
		                    	<img class="people-pic" src="/assets/img/speakers/masoud.jpeg" target="_blank" />
		                    
		                </p>
		            </div>
		        </td>
    		    <td style="border: none;">
		            <div class="people-name text-center">
		            	<!-- Speaker name (link to webpage if provided) -->
		            	
		            		<b><a href="https://scholar.google.com/citations?user=TSG6jcMAAAAJ&amp;hl=en" target="_blank">Masoud Mansoury</a></b>
		            	
		                <br />
		                <!-- Speaker affiliation (if provided) -->
		                
		                	University of Amsterdam
		                
		                <!-- Additional speaker affiliation (if provided) -->
		                
		            </div>
		        </td>
		        <td style="border: none;">
		        	<div class="people-name text-center">
		        		<!-- Whatever you write below will show up as the speaker's bio -->

<p>Masoud Mansoury is a postdoctoral researcher at Amsterdam Machine Learning Lab at University of Amsterdam, Netherlands. He is also a member of Discovery Lab collaborating with the Data Science team at Elsevier Company in the area of recommender systems. Masoud received his PhD in Computer and Information Science from Eindhoven University of Technology, Netherlands, in 2021. He has published his research works in top conferences such as FAccT, RecSys, and CIKM. His research interests include recommender systems, algorithmic bias, and contextual bandits.</p>

		        	</div>
		        </td>
		    </tr>
	    
		    <tr style="border: none;">
		        <td style="border: none;">
		            <div class="col-xs-6">
		                <p align="center">
		                	
		                    	<img class="people-pic" src="/assets/img/speakers/barocas.jpeg" target="_blank" />
		                    
		                </p>
		            </div>
		        </td>
    		    <td style="border: none;">
		            <div class="people-name text-center">
		            	<!-- Speaker name (link to webpage if provided) -->
		            	
		            		<b><a href="http://solon.barocas.org/" target="_blank">Solon Barocas</a></b>
		            	
		                <br />
		                <!-- Speaker affiliation (if provided) -->
		                
		                	Microsoft, Cornell University
		                
		                <!-- Additional speaker affiliation (if provided) -->
		                
		            </div>
		        </td>
		        <td style="border: none;">
		        	<div class="people-name text-center">
		        		<!-- Whatever you write below will show up as the speaker's bio -->

<p>Solon Barocas is a Principal Researcher in the New York City lab of Microsoft Research and an Adjunct Assistant Professor in the Department of Information Science at Cornell University. His research explores ethical and policy issues in artificial intelligence, particularly fairness in machine learning, methods for bringing accountability to automated decision-making, and the privacy implications of inference.</p>

		        	</div>
		        </td>
		    </tr>
	    
	  </table>
  </div>
</figure>

<h3 id="news">News</h3>

<ul>
  <li>Accepted papers and talks are now <a href="papers">visible</a>.</li>
  <li>Camera-ready and video submission deadlines are updated in <a href="submit">important dates</a>.</li>
  <li>Notifications for Accepted papers are out.</li>
  <li>Call for papers is out! Last date to submit is <strong>May 31, 2022</strong>. Please check <a href="submit">instructions</a> on how to submit.</li>
</ul>

<!--
* [Schedule](schedule) updated. 
* Accepted papers and reviews [available](papers)
* Thank you [reviewers](people/#reviewers)! 
-->

<h3 id="contact-us">Contact us</h3>

<p>The organizers may be reached at <code class="language-plaintext highlighter-rouge">responsibledecisionmaking &lt;AT&gt; gmail &lt;DOT&gt; com</code></p>

<p><a href="https://twitter.com/responsibledec1">Follow us on Twitter</a>!</p>

<h3 id="related-past-workshops">Related past workshops</h3>

<ol>
  <li><a href="https://iclrsrml.github.io/">Socially Responsible Machine Learning (SRML)</a> - ICLR, 2022</li>
  <li><a href="https://icmlsrml2021.github.io/">Socially Responsible Machine Learning</a> - ICML, 2021</li>
  <li><a href="https://sites.google.com/view/strategicml/">Learning in Presence of Strategic Behavior</a> - NeurIPS, 2021</li>
  <li><a href="https://sites.google.com/view/rai-workshop/home">Workshop on Responsible AI </a> - ICLR, 2021</li>
  <li><a href="https://dynamicdecisions.github.io/">Workshop on Consequential Decision Making in Dynamic Environments</a> - NeurIPS, 2020</li>
  <li><a href="https://sites.google.com/view/icml-law-and-ml-2020/home">Law &amp; Machine Learning (LML)</a> - ICML, 2020</li>
  <li><a href="https://sites.google.com/view/whi2020">Workshop on Human Interpretability in Machine Learning (WHI)</a> - ICML, 2020</li>
  <li><a href="https://sites.google.com/view/RL4RealLife">Reinforcement Learning for Real Life Workshop</a> - ICML, 2021</li>
  <li><a href="https://sites.google.com/view/safe-robust-control/home">Safe and Robust Control of Uncertain Systems</a> - NeurIPS, 2021</li>
  <li><a href="https://perls-workshop.github.io/">Political Economy of Reinforcement Learning (PERLS) Workshop
</a> - NeurIPS, 2021</li>
</ol>
:ET