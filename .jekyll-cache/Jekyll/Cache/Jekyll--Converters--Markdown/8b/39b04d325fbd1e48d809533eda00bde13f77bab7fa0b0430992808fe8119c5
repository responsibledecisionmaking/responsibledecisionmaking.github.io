I"Ö<!-- > "All communication must lead to change." -- Aristotle -->

<!-- 
<figure>
	<div style="text-align:center">
		<img src="assets/img/banner.png" alt="A logo image for the rethinking ML papers workshop, designed by Falaah Arif Khan" />
		<figcaption>Image Credits: <a href="https://falaaharifkhan.github.io/research/">Falaah Arif Khan</a></figcaption>
	</div>
</figure>
-->

<!-- > Join us at our ICLR workshop on **Friday, May 7 2021** (0800 hrs - 1500 hrs Eastern Daylight Time) -->

<p>Algorithmic decision-making systems are increasingly used in sensitive applications such as advertising, resume reviewing, employment, credit lending, policing, criminal justice, and beyond. The long-term promise of these approaches is to automate, augment and/or eventually improve on the human decisions which can be biased or unfair, by leveraging the potential of machine learning to make decisions supported by historical data. Unfortunately, there is a growing body of evidence showing that the current machine learning technology is vulnerable to privacy or security attacks, lacks interpretability, or reproduces (and even exacerbates) historical biases or discriminatory behaviors against certain social groups.</p>

<p>Most of the literature on building socially responsible algorithmic decision-making systems focus on a static scenario where algorithmic decisions do not change the data distribution. However, real-world applications involve nonstationarities and feedback loops that must be taken into account to measure and mitigate fairness in the long-term. These feedback loops involve the learning process which may be biased because of insufficient exploration, or changes in the environmentâ€™s dynamics due to strategic responses of the various stakeholders. From a machine learning perspective, these sequential processes are primarily studied through counterfactual analysis and reinforcement learning.</p>

<p>The purpose of this workshop is to bring together researchers from both industry and academia working on the full spectrum of responsible decision-making in dynamic environments, from theory to practice. In particular, we encourage submissions on the following topics:</p>
<ul>
  <li>Fairness,</li>
  <li>Privacy and security,</li>
  <li>Robustness,</li>
  <li>Conservative and safe algorithms,</li>
  <li>Explainability and interpretability.</li>
</ul>

<h3 id="news">News</h3>

<ul>
  <li>Call for papers is out! Last date to submit is <strong>June 3, 2022</strong>. Please check <a href="submit">instructions</a> on how to submit.</li>
</ul>

<!--
* [Schedule](schedule) updated. 
* Accepted papers and reviews [available](papers)
* Thank you [reviewers](people/#reviewers)! 
-->

<h3 id="contact-us">Contact us</h3>

<p>The organizers may be reached at <code class="language-plaintext highlighter-rouge">responsibledecisionmaking &lt;AT&gt; gmail.com</code></p>

<p><a href="https://twitter.com/responsibledec1">Follow us on Twitter</a>!</p>

<h3 id="related-past-workshops">Related past workshops</h3>

<ol>
  <li><a href="https://iclrsrml.github.io/">Socially Responsible Machine Learning (SRML)</a> - ICLR, 2022</li>
  <li><a href="https://icmlsrml2021.github.io/">Socially Responsible Machine Learning</a> - ICML, 2021</li>
  <li><a href="https://sites.google.com/view/strategicml/">Learning in Presence of Strategic Behavior</a> - NeurIPS, 2021</li>
  <li><a href="https://sites.google.com/view/rai-workshop/home">Workshop on Responsible AI </a> - ICLR 2021</li>
  <li><a href="https://dynamicdecisions.github.io/">Workshop on Consequential Decision Making in Dynamic Environments</a> - NeurIPS, 2020</li>
  <li><a href="https://sites.google.com/view/icml-law-and-ml-2020/home">Law &amp; Machine Learning (LML)</a> - ICML 2020</li>
  <li><a href="https://sites.google.com/view/whi2020">Workshop on Human Interpretability in Machine Learning (WHI)</a> - ICML 2020</li>
  <li><a href="https://sites.google.com/view/RL4RealLife">Reinforcement Learning for Real Life Workshop</a> - ICML 2021</li>
  <li><a href="https://sites.google.com/view/safe-robust-control/home">Safe and Robust Control of Uncertain Systems</a> - NeurIPS 2021</li>
  <li><a href="https://perls-workshop.github.io/">Political Economy of Reinforcement Learning (PERLS) Workshop
</a> - NeurIPS 2021</li>
</ol>
:ET